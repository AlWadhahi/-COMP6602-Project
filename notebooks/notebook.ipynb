{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../car-damage-dataset/data1a/training'\n",
    "test_path = '../car-damage-dataset/data1a/validation'\n",
    "\n",
    "train_files = glob(train_path + '/*/*.jp*g')\n",
    "test_files = glob(test_path + '/*/*.jp*g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00-damage', '01-whole']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_vectors(images):\n",
    "    \"\"\"\n",
    "    helper function to convert glob to vector\n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_images = len(images)\n",
    "    w = 256\n",
    "    h = 256\n",
    "    channels = 3\n",
    "\n",
    "    X = np.empty(shape= (number_of_images, w*h*channels))\n",
    "\n",
    "    labels = np.empty(number_of_images)\n",
    "\n",
    "    for i, sample in enumerate(images):\n",
    "\n",
    "        image_vector = Image.open(sample) \n",
    "        \n",
    "        # Resize to 256 x 256\n",
    "        resized_image_vector = np.array(image_vector.resize((256, 256)))\n",
    "\n",
    "        # Scale to [0,1]\n",
    "        norm_image_vector = resized_image_vector / 255\n",
    "\n",
    "        flattened_image_vector = norm_image_vector.flatten()\n",
    "\n",
    "        X[i] = np.pad(flattened_image_vector, (0, (w*h*channels) - len(flattened_image_vector) ))\n",
    "\n",
    "        if sample.split(\"\\\\\")[-2] == \"00-damage\":\n",
    "            labels[i] = 0\n",
    "\n",
    "        else:\n",
    "            labels[i] = 1\n",
    "\n",
    "    return X, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pad([1,2,3], (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1, y1 = load_images_as_vectors(train_files)\n",
    "\n",
    "X2, y2 = load_images_as_vectors(test_files)\n",
    "\n",
    "X = np.append(X1, X2, axis=0)\n",
    "\n",
    "y = np.append(y1, y2, axis=0)\n",
    "\n",
    "# Set the n_components=3\n",
    "principal=PCA(n_components=30)\n",
    "\n",
    "principal.fit(X)\n",
    "\n",
    "X=principal.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 20\n",
      "max_resources_: 1840\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 840\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 840 candidates, totalling 4200 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 280\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 280 candidates, totalling 1400 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 94\n",
      "n_resources: 180\n",
      "Fitting 5 folds for each of 94 candidates, totalling 470 fits\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "\n",
    "param_grid = { \n",
    "    'C':[0.01,0.1,1,100,1000,10000],\n",
    "    'kernel':['rbf','poly','sigmoid','linear'],\n",
    "    'degree':[2,3,4,5,6],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "    }\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "# model = GridSearchCV(svc,param_grid, n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "model = HalvingGridSearchCV(svc, param_grid, n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(\n",
    "   pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    index = ['Damaged', 'Non-damaged'],\n",
    "    columns = ['Damaged', 'Non-damaged']\n",
    "), \n",
    "    annot=True, \n",
    "    fmt='g', \n",
    "    cmap='Blues'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: AUC/ROC\n",
    "# TODO: Precision\n",
    "# TODO: Recall\n",
    "# TODO: F1-score\n",
    "# TODO: Confusion Matrix\n",
    "# TODO: Highlight pixels"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7d28c78db0730d805e68481932e7e809744adca621765ff38b402e91a451557"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('comp6602-project-zTWnP2sZ-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
